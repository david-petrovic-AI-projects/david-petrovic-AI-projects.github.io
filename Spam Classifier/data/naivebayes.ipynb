{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Na√Øve Bayes ‚Äì Additional Advice\n",
    "This notebook guides you through multiple steps you can follow to create a na√Øve Bayes classifier. After following these steps you will still need to collate and move your code into the main assignment notebook file so that it meets the required format.\n",
    "\n",
    "Read each step (including the maths!) carefully.\n",
    "\n",
    "You can implement a na√Øve Bayes classifier without following this advice.\n",
    "\n",
    "This notebook will not be graded and does not need to be submitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the spam training data set: (1000, 55)\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [1. 1. 1. ... 1. 1. 0.]\n",
      " [1. 0. 0. ... 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "training_spam = np.loadtxt(open(\"training_spam.csv\"), delimiter=\",\")\n",
    "print(\"Shape of the spam training data set:\", training_spam.shape)\n",
    "print(training_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model:  na√Øve Bayes\n",
    "Your [na√Øve Bayes](https://en.wikipedia.org/wiki/Naive_Bayes_classifier) classifier will distinguish between two classes:\n",
    "\n",
    "* $C = 1$ for spam messages\n",
    "* $C = 0$ for ham messages\n",
    "\n",
    "\n",
    "The classifier builds a model for the probability $p(C=c\\ |\\ \\text{message})$ that a given message belongs to a certain class. A new message is then classified based on the Bayesian *maximum a posteriori* estimate\n",
    "$\\require{color}$\n",
    "\\begin{equation}\n",
    "\\hat{c} = \\underset{c \\in \\{0,1\\}}{\\operatorname{argmax}} \\  \\textcolor{blue}{p(C=c\\ |\\ \\text{message})}.\n",
    "\\end{equation}\n",
    "Using Bayes' rule we can write\n",
    "\n",
    "\\begin{equation}\n",
    "p(C=c\\ |\\ \\text{message}) = \\frac{p(\\text{message}\\ |\\ C=c)p(C=c)}{p(\\text{message}\\ |\\ C=1)p(C=1) + p(\\text{message}\\ |\\ C=0)p(C=0)}.  \\quad \\quad \n",
    "\\end{equation}\n",
    "\n",
    "The denominator is the same for both classes and we can thus drop it to get\n",
    "\n",
    "\\begin{equation}\n",
    "\\textcolor{blue}{p(C=c\\ |\\ \\text{message})} \\propto \\textcolor{orange}{p(\\text{message}\\ |\\ C=c)}\\textcolor{green}{p(C=c)},\n",
    "\\end{equation}\n",
    "\n",
    "where $\\propto$ means \"proportional to\". The class priors $\\textcolor{green}{p(C=c)}$ can be computed directly (you will do so in exercise A) but we need to further simplify $\\textcolor{orange}{p(\\text{message} \\ |\\ C=c)}$.\n",
    "\n",
    "\n",
    "### Choice of the event model: *Multinomial* na√Øve Bayes\n",
    "\n",
    "Different na√Øve Bayes models differ in their distributional assumptions about $\\textcolor{orange}{p(\\text{message}\\ |\\ C=c)}$. We represent a message using a **binary** [bag-of-words](https://en.wikipedia.org/wiki/Bag-of-words_model) model. Specifically, a message is represented as a set of $k$ keywords, that is, $message = (w_1, ..., w_k)$, where $w_i = 1$ if the  keyword $w_i$ appears in the message and $w_i = 0$ otherwise.\n",
    "\n",
    "We assume that the $p(w_1, ..., w_k |\\ C=c)$ follows a [multinomial distribution](https://en.wikipedia.org/wiki/Multinomial_distribution) for each class. Don't let the name scare you, this model simply assigns probabilities to different counts of events with multiple outcomes. So for example: \"I roll a biased six-sided die six times, what is the probability that I get each side occurring exactly once\" is a question that can be answered with a multinomial distribution. You don't need to understand all of the equations on the Wikipedia page.\n",
    "\n",
    "Intuitively, the multinomial distribution assumes that the words of a message were \"drawn\" independently from a bag of $k$ different words. Depending on the class membership $c$, each keyword $w$ has a probability $\\theta_{c, w}$ of being drawn. For example,\n",
    "\n",
    "* $\\theta_{spam, w}$ will have high value for $w \\in \\{$bank, transfer, buy,... $\\}$.\n",
    "* $\\theta_{ham, w}$ will have high value for $w \\in \\{$paper, conference, proposal, experiment,... $\\}$, if the training data was mostly gathered from emails of researchers.\n",
    "\n",
    "Under these assumptions, the likelihood of a message, given that it belongs to class $c$, is then proportional to\n",
    "\\begin{equation}\n",
    "\\textcolor{orange}{p(\\text{message}\\ |\\ C=c)} \\propto \\prod_{i = 1}^k  \\left(\\textcolor{brown}{\\theta_{c, w_i}} \\right)^{w_i}.\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "The parameters $\\textcolor{brown}{\\theta_{c, w}}$ are estimated by counting the relative frequencies in the training data. Use **Laplace-smoothing** with $\\alpha = 1$ (add-one smoothing), that is,\n",
    "\\begin{equation}\n",
    "\\textcolor{brown}{\\theta_{c, w}} = \\frac{n_{c, w} + \\alpha}{n_{c} + k \\alpha},\n",
    "\\end{equation}\n",
    "where $n_{c, w}$ is the number of times the keyword $w$ appears in messages of class $c$ in the training set and $n_{c}$ is the total count of keywords for all messages of class $c$, that is, $n_{c} = \\sum_w n_{c, w}$.\n",
    "\n",
    "\n",
    "\n",
    "We are now finally able to rewrite the *maximum a posteriori* estimate in a form that is easy to compute:\n",
    "\\begin{equation}\n",
    "\\hat{c} = \\underset{c \\in \\{0,1\\}}{\\operatorname{argmax}} \\ \\left[ \\textcolor{green}{p(C=c)}   \\prod_{i = 1}^k  \\left(\\textcolor{brown}{\\theta_{c, w_i}} \\right)^{w_i}\\right].\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "#### Increasing numerical stability\n",
    "We can increase the numerical stability of the algorithm by taking logarithms of the posterior distributions, that is,\n",
    "\\begin{equation}\n",
    "\\hat{c} = \\underset{c \\in \\{0,1\\}}{\\operatorname{argmax}} \\ \\log \\left( \\textcolor{green}{p(C=c)}   \\prod_{i = 1}^k  \\left(\\textcolor{brown}{\\theta_{c, w_i}}\\right)^{w_i} \\right) \\\\\n",
    " = \\underset{c \\in \\{0,1\\}}{\\operatorname{argmax}} \\ \\left[ \\log( \\textcolor{green}{p(C=c)}) + \\sum_{i = 1}^k w_i \\ \\log \\left(\\textcolor{brown}{\\theta_{c, w_i}} \\right) \\right].\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A: Estimate class priors\n",
    "\n",
    "Define a function called `estimate_log_class_priors()` that takes as input a data set with binary response variable (0s and 1s) in the left-most column and returns a numpy array containing the **the logarithm** of the empirical class priors $\\textcolor{green}{p(C=c)}$ for $c \\in \\{0, 1\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def estimate_log_class_priors(data):\n",
    "    \"\"\"\n",
    "    Given a data set with binary response variable (0s and 1s) in the\n",
    "    left-most column, calculate the logarithm of the empirical class priors,\n",
    "    that is, the logarithm of the proportions of 0s and 1s:\n",
    "        log(p(C=0)) and log(p(C=1))\n",
    "\n",
    "    :param data: a two-dimensional numpy-array with shape = [n_samples, 1 + n_features]\n",
    "                 the first column contains the binary response (coded as 0s and 1s).\n",
    "\n",
    "    :return log_class_priors: a numpy array of length two\n",
    "    \"\"\"\n",
    "    # Iterate through all the training data, to determine p(C=0) p(C=1)\n",
    "    count_zero = 0\n",
    "    count_one = 0\n",
    "    for i, element in enumerate(data):\n",
    "        if data[i][0] == 0:\n",
    "            count_zero += 1\n",
    "        else:\n",
    "            count_one += 1\n",
    "    # Calculate the percentages\n",
    "    p_count_one = count_one / len(data)\n",
    "    p_count_zero = count_zero / len(data)\n",
    "    # Calculate the log values\n",
    "    spam_log_p_one = math.log10(p_count_one)\n",
    "    ham_log_p_zero = math.log10(p_count_zero)\n",
    "    return np.array([spam_log_p_one, ham_log_p_zero])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e56af38a496339fb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result [-0.41228903 -0.21253953]\n"
     ]
    }
   ],
   "source": [
    "# You can use this cell to check whether the returned objects of your function are of the right data type.\n",
    "log_class_priors = estimate_log_class_priors(training_spam)\n",
    "print(\"result\", log_class_priors)\n",
    "\n",
    "# Check length\n",
    "assert(len(log_class_priors) == 2)\n",
    "\n",
    "# Check whether the returned object is a numpy.ndarray\n",
    "assert(isinstance(log_class_priors, np.ndarray))\n",
    "\n",
    "# Check wehther the values of this numpy.array are floats.\n",
    "assert(log_class_priors.dtype == float)\n",
    "\n",
    "# Check wehther the values are both negative (the logarithm of a probability 0 < p < 1 should be negative).\n",
    "assert(np.all(log_class_priors < 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B: Estimate class-conditional likelihoods\n",
    "Define a function called `estimate_log_class_conditional_likelihoods()` that takes as input a data set with binary response variable (0s and 1s) in the left-most column and returns **the logarithm** of the empirical class-conditional likelihoods $\\log \\left(\\textcolor{brown}{\\theta_{c, w_i}} \\right)$ for all words $w_i$ and both classes ($c \\in {0, 1}$). These parameters should be returned in a two-dimensional numpy-array with shape = `[num_classes, num_features]`.\n",
    "\n",
    "Assume a multinomial event model and use Laplace smoothing with $\\alpha = 1$. \n",
    "\n",
    "Hint: many `numpy`-functions contain an `axis` argument. If you specify `axis=0`, you can perform column-wise (that is, feature-wise!) computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_log_class_conditional_likelihoods(data, alpha=1.0):\n",
    "    \"\"\"\n",
    "    Given a data set with binary response variable (0s and 1s) in the\n",
    "    left-most column and binary features (words), calculate the empirical\n",
    "    class-conditional likelihoods, that is,\n",
    "    log(P(w_i | c)) for all features w_i and both classes (c in {0, 1}).\n",
    "\n",
    "    Assume a multinomial feature distribution and use Laplace smoothing\n",
    "    if alpha > 0.\n",
    "\n",
    "    :param data: a two-dimensional numpy-array with shape = [n_samples, 1 + n_features]\n",
    "\n",
    "    :return theta:\n",
    "        a numpy array of shape = [2, n_features]. theta[j, i] corresponds to the\n",
    "        logarithm of the probability of feature i appearing in a sample belonging \n",
    "        to class j.\n",
    "    \"\"\"\n",
    "    # k defined generally, so boosting can be used later\n",
    "    k = data.shape[1]-1\n",
    "    \n",
    "    # Laplace smoothing, could be more than 1\n",
    "    alpha = 1\n",
    "    \n",
    "    # Returns numpy array with only spam rows, or only ham rows\n",
    "    spam_arr = data[data[:, 0] == 1]\n",
    "    ham_arr  = data[data[:, 0] == 0]\n",
    "    \n",
    "    # Calculates nc for both classes\n",
    "    spam_nc = spam_arr[:, 1:].sum()\n",
    "    ham_nc = ham_arr[:, 1:].sum()\n",
    "    \n",
    "\n",
    "    # Calculates ùúÉc,w for both classes\n",
    "    theta_spam = spam_arr[:, 1:].sum(axis=0)\n",
    "    theta_ham = ham_arr[:, 1:].sum(axis=0)\n",
    "    # Combine results\n",
    "    theta_spam = (theta_spam + alpha) / (spam_nc + (alpha * k))\n",
    "    theta_spam = np.log10(theta_spam)\n",
    "    theta_ham = (theta_ham + alpha) / (ham_nc + (alpha * k))\n",
    "    theta_ham = np.log10(theta_ham)\n",
    "    return np.array([theta_spam, theta_ham])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-851fa744923a9bba",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.53871733 -1.62324929 -1.34054668 -2.86093662 -1.30120094 -1.49920878\n",
      "  -1.50737757 -1.55990663 -1.64157667 -1.44121684 -1.64157667 -1.32042839\n",
      "  -1.60912465 -2.01583858 -1.96567197 -1.36164239 -1.47558574 -1.57249575\n",
      "  -1.16445542 -1.76155199 -1.20772411 -2.42728106 -1.60223094 -1.51291006\n",
      "  -2.62685341 -2.75179215 -3.40500467 -2.92788341 -3.00706466 -3.10397467\n",
      "  -3.40500467 -3.70603466 -2.59209131 -3.70603466 -2.50191468 -2.25887663\n",
      "  -2.32582342 -2.86093662 -2.47558574 -1.98175879 -3.70603466 -3.22891341\n",
      "  -2.30809465 -3.00706466 -1.67665088 -2.59209131 -2.92788341 -3.10397467\n",
      "  -1.89985469 -1.31333771 -2.14973216 -1.18620667 -1.31686858 -1.67665088]\n",
      " [-1.66705999 -1.91111393 -1.46599484 -3.39541377 -1.55031573 -1.89026379\n",
      "  -2.58250041 -2.08365991 -1.96405    -1.68784359 -2.11666017 -1.27483984\n",
      "  -1.78795874 -2.24928573 -2.61726252 -1.91111393 -1.88353041 -1.78795874\n",
      "  -1.15988532 -2.58250041 -1.37422447 -2.91829251 -2.46599484 -2.58250041\n",
      "  -1.33095578 -1.46091532 -1.48159992 -1.71417253 -1.78795874 -1.73740237\n",
      "  -1.90405207 -2.01520253 -1.86393485 -2.00624768 -1.71872016 -1.68360654\n",
      "  -1.4509311  -2.61726252 -1.85134572 -1.91111393 -2.24928573 -1.81563017\n",
      "  -1.88353041 -1.90405207 -1.43876519 -1.67525446 -2.61726252 -2.12824204\n",
      "  -1.64336532 -1.16496485 -1.74220125 -1.44602376 -1.89026379 -1.97216789]]\n"
     ]
    }
   ],
   "source": [
    "# You can use this cell to check whether the returned objects of your function are of the right data type.\n",
    "log_class_conditional_likelihoods = estimate_log_class_conditional_likelihoods(training_spam, alpha=1.0)\n",
    "print(log_class_conditional_likelihoods)\n",
    "\n",
    "# Check data type(s)\n",
    "assert(isinstance(log_class_conditional_likelihoods, np.ndarray))\n",
    "\n",
    "# Check shape of numpy array\n",
    "assert(log_class_conditional_likelihoods.shape == (2, 54))\n",
    "\n",
    "# Check data type of array elements\n",
    "assert(log_class_conditional_likelihoods.dtype == float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part  C: Classify e-mails\n",
    "\n",
    "Having calculated the log class priors and the log class-conditional likelihoods for a given training set, define a function called `predict()`that takes a data set of new messages as input and predicts for each message whether it is spam or not. Note that the input should **not** contain a response variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(new_data, log_class_priors, log_class_conditional_likelihoods):\n",
    "    \"\"\"\n",
    "    Given a new data set with binary features, predict the corresponding\n",
    "    response for each instance (row) of the new_data set.\n",
    "\n",
    "    :param new_data: a two-dimensional numpy-array with shape = [n_test_samples, n_features].\n",
    "    :param log_class_priors: a numpy array of length 2.\n",
    "    :param log_class_conditional_likelihoods: a numpy array of shape = [2, n_features].\n",
    "        theta[j, i] corresponds to the logarithm of the probability of feature i appearing\n",
    "        in a sample belonging to class j.\n",
    "    :return class_predictions: a numpy array containing the class predictions for each row\n",
    "        of new_data.\n",
    "    \"\"\"\n",
    "    spam_predict = log_class_priors[0] + np.sum(new_data * log_class_conditional_likelihoods[0], axis=1)\n",
    "    ham_predict = log_class_priors[1] + np.sum(new_data * log_class_conditional_likelihoods[1], axis=1)\n",
    "    class_predictions = np.where(spam_predict > ham_predict, 1, 0)\n",
    "    return class_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4c8adaa150209180",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# You can use this cell to check whether the returned objects of your function are of the right data type.\n",
    "class_predictions = predict(training_spam[:, 1:], log_class_priors, log_class_conditional_likelihoods)\n",
    "\n",
    "# Check data type(s)\n",
    "assert(isinstance(class_predictions, np.ndarray))\n",
    "\n",
    "# Check shape of numpy array\n",
    "assert(class_predictions.shape == (1000,))\n",
    "\n",
    "# Check data type of array elements\n",
    "assert(np.all(np.logical_or(class_predictions == 0, class_predictions == 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test your `predict` function by classifying messages. You can do this to the *training* data, but you should also try it on the *testing* data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the training set: 0.892\n"
     ]
    }
   ],
   "source": [
    "# Check accuracy\n",
    "true_classes = training_spam[:, 0]\n",
    "training_set_accuracy = np.mean(np.equal(class_predictions, true_classes))\n",
    "print(f\"Accuracy on the training set: {training_set_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you are done, you can move the code into the main assignment notebook.\n",
    "\n",
    "One way to do this is to follow the rough structure of the class that already exists in that notebook. You can use the `train` method to pass in the data and perform all of the steps before the prediction. You should store data in instance variables, e.g. `self.log_class_priors` and `self.log_class_conditional_likelihoods`. This means that then you can set up the `predict` method to match the one above without needing to pass in the additional variables. **Important:** the predict method must only take a single variable as a parameter (the one called `new_data`) in the skeleton code above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
